{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35e6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1453181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eab6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('meow').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdf73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test3.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98d9ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "|Name | Age|Experience|Salary|\n",
      "+-----+----+----------+------+\n",
      "|Aryan|  20|        34| 69000|\n",
      "|Grace|  19|        29|220000|\n",
      "|Sneha|  19|        78|170000|\n",
      "| Rene|  33|        22| 22000|\n",
      "| John|  20|        11| 18000|\n",
      "| Isen|  22|        45| 90000|\n",
      "| Arlo|  19|        12|120000|\n",
      "|Blyke|  21|        22|200000|\n",
      "| null|null|      null|230000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c25edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  20|        34| 69000|\n",
      "|  19|        29|220000|\n",
      "|  19|        78|170000|\n",
      "|  33|        22| 22000|\n",
      "|  20|        11| 18000|\n",
      "|  22|        45| 90000|\n",
      "|  19|        12|120000|\n",
      "|  21|        22|200000|\n",
      "|null|      null|230000|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop the columns\n",
    "df_pyspark.drop('Name ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da635fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "|Name | Age|Experience|Salary|\n",
      "+-----+----+----------+------+\n",
      "|Aryan|  20|        34| 69000|\n",
      "|Grace|  19|        29|220000|\n",
      "|Sneha|  19|        78|170000|\n",
      "| Rene|  33|        22| 22000|\n",
      "| John|  20|        11| 18000|\n",
      "| Isen|  22|        45| 90000|\n",
      "| Arlo|  19|        12|120000|\n",
      "|Blyke|  21|        22|200000|\n",
      "| null|null|      null|230000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3adbf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "|Name |Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Aryan| 20|        34| 69000|\n",
      "|Grace| 19|        29|220000|\n",
      "|Sneha| 19|        78|170000|\n",
      "| Rene| 33|        22| 22000|\n",
      "| John| 20|        11| 18000|\n",
      "| Isen| 22|        45| 90000|\n",
      "| Arlo| 19|        12|120000|\n",
      "|Blyke| 21|        22|200000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()## dropped the null ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d98f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "|Name |Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Aryan| 20|        34| 69000|\n",
      "|Grace| 19|        29|220000|\n",
      "|Sneha| 19|        78|170000|\n",
      "| Rene| 33|        22| 22000|\n",
      "| John| 20|        11| 18000|\n",
      "| Isen| 22|        45| 90000|\n",
      "| Arlo| 19|        12|120000|\n",
      "|Blyke| 21|        22|200000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## any == how\n",
    "df_pyspark.na.drop(how='any').show() ## even if one null is present we drop that row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89d9423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "|Name | Age|Experience|Salary|\n",
      "+-----+----+----------+------+\n",
      "|Aryan|  20|        34| 69000|\n",
      "|Grace|  19|        29|220000|\n",
      "|Sneha|  19|        78|170000|\n",
      "| Rene|  33|        22| 22000|\n",
      "| John|  20|        11| 18000|\n",
      "| Isen|  22|        45| 90000|\n",
      "| Arlo|  19|        12|120000|\n",
      "|Blyke|  21|        22|200000|\n",
      "| null|null|      null|230000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## all==how\n",
    "df_pyspark.na.drop(how='all').show()##drops only if all are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f2e447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "|Name |Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Aryan| 20|        34| 69000|\n",
      "|Grace| 19|        29|220000|\n",
      "|Sneha| 19|        78|170000|\n",
      "| Rene| 33|        22| 22000|\n",
      "| John| 20|        11| 18000|\n",
      "| Isen| 22|        45| 90000|\n",
      "| Arlo| 19|        12|120000|\n",
      "|Blyke| 21|        22|200000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshold\n",
    "df_pyspark.na.drop(how='any',thresh=2).show()##atleast 2 null values we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "402b2aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "|Name |Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Aryan| 20|        34| 69000|\n",
      "|Grace| 19|        29|220000|\n",
      "|Sneha| 19|        78|170000|\n",
      "| Rene| 33|        22| 22000|\n",
      "| John| 20|        11| 18000|\n",
      "| Isen| 22|        45| 90000|\n",
      "| Arlo| 19|        12|120000|\n",
      "|Blyke| 21|        22|200000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## subset\n",
    "df_pyspark.na.drop(how='any',subset=['Experience']).show()\n",
    "## see if we have a nan value in the exp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ab1b4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "|Name | Age|Experience|Salary|\n",
      "+-----+----+----------+------+\n",
      "|Aryan|  20|        34| 69000|\n",
      "|Grace|  19|        29|220000|\n",
      "|Sneha|  19|        78|170000|\n",
      "| Rene|  33|        22| 22000|\n",
      "| John|  20|        11| 18000|\n",
      "| Isen|  22|        45| 90000|\n",
      "| Arlo|  19|        12|120000|\n",
      "|Blyke|  21|        22|200000|\n",
      "| null|null|      null|230000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##filling the missing  values\n",
    "df_pyspark.na.fill('NA','Experience').show()#should fill nulls of experience \n",
    "#with NA dk why its not happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b254aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "|Name | Age|Experience|Salary|\n",
      "+-----+----+----------+------+\n",
      "|Aryan|  20|        34| 69000|\n",
      "|Grace|  19|        29|220000|\n",
      "|Sneha|  19|        78|170000|\n",
      "| Rene|  33|        22| 22000|\n",
      "| John|  20|        11| 18000|\n",
      "| Isen|  22|        45| 90000|\n",
      "| Arlo|  19|        12|120000|\n",
      "|Blyke|  21|        22|200000|\n",
      "| null|null|      null|230000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52410ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use imputer functions to replace the nulls with means etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
